{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "428d0682-2fae-4de8-9196-7e5f02deb581",
      "metadata": {
        "id": "428d0682-2fae-4de8-9196-7e5f02deb581"
      },
      "source": [
        "# AI - CA4 - Machine Learning - Ava KazemiNejad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8647b8f2-e6c8-457a-bddf-fd0763d9edf8",
      "metadata": {
        "id": "8647b8f2-e6c8-457a-bddf-fd0763d9edf8"
      },
      "source": [
        "## Goal\n",
        "In this assignment, we're going to explore diabetes dataset and train some models to predict songs' genres. Let's start!\n",
        "\n",
        "## Overall approach\n",
        "In order to train the models, we first visualize data. Then we preprocess it and finally we'll tune hyperparameters, train the model and evaluate the results. Let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f9f9f0aa-7a24-4435-a856-c54929787aaf",
      "metadata": {
        "id": "f9f9f0aa-7a24-4435-a856-c54929787aaf"
      },
      "outputs": [],
      "source": [
        "# Libraries for preprocessing\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Scientific libraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7696616-9980-4a2f-9637-9647a75cea5e",
      "metadata": {
        "id": "e7696616-9980-4a2f-9637-9647a75cea5e"
      },
      "source": [
        "## Phase 0: EDA and visualization\n",
        "First we need to read the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b028e9b2-1fd3-41cf-947e-373dbd1d370d",
      "metadata": {
        "id": "b028e9b2-1fd3-41cf-947e-373dbd1d370d"
      },
      "outputs": [],
      "source": [
        "db = pd.read_csv('/content/diabetes.csv')\n",
        "\n",
        "data = db.iloc[:, :-1]\n",
        "label = db.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "248f1b3f-d4e3-4220-94ad-74c71c762c85",
      "metadata": {
        "id": "248f1b3f-d4e3-4220-94ad-74c71c762c85"
      },
      "source": [
        "We'll explore the data using describe and info methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e6aa9409-6560-4230-a44d-798ec1297acc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "e6aa9409-6560-4230-a44d-798ec1297acc",
        "outputId": "66b3b57e-be6a-43de-9db2-dc932ae6b342"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Pregnancies      Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
              "count   635.000000   654.000000     680.000000     624.000000  680.000000   \n",
              "mean      3.700787   113.422018      68.786765      20.386218   80.123529   \n",
              "std       3.518126   202.816831      19.724841      15.987049  115.681140   \n",
              "min     -22.000000 -5000.000000      -2.000000       0.000000    0.000000   \n",
              "25%       1.000000    99.000000      62.000000       0.000000    0.000000   \n",
              "50%       3.000000   117.000000      72.000000      23.000000   34.000000   \n",
              "75%       6.000000   140.750000      80.000000      32.000000  129.250000   \n",
              "max      17.000000   199.000000     122.000000      99.000000  846.000000   \n",
              "\n",
              "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
              "count  684.000000                590.000000  655.000000  768.000000  \n",
              "mean    32.083626                  0.466676   33.157252    0.348958  \n",
              "std      7.800741                  0.322408   13.829831    0.476951  \n",
              "min      0.000000                  0.078000 -150.000000    0.000000  \n",
              "25%     27.375000                  0.243250   24.000000    0.000000  \n",
              "50%     32.300000                  0.368000   29.000000    0.000000  \n",
              "75%     36.600000                  0.611500   41.000000    1.000000  \n",
              "max     67.100000                  2.329000   81.000000    1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e645f850-9e2a-480f-96dd-8ba4bd86b8df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>635.000000</td>\n",
              "      <td>654.000000</td>\n",
              "      <td>680.000000</td>\n",
              "      <td>624.000000</td>\n",
              "      <td>680.000000</td>\n",
              "      <td>684.000000</td>\n",
              "      <td>590.000000</td>\n",
              "      <td>655.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.700787</td>\n",
              "      <td>113.422018</td>\n",
              "      <td>68.786765</td>\n",
              "      <td>20.386218</td>\n",
              "      <td>80.123529</td>\n",
              "      <td>32.083626</td>\n",
              "      <td>0.466676</td>\n",
              "      <td>33.157252</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.518126</td>\n",
              "      <td>202.816831</td>\n",
              "      <td>19.724841</td>\n",
              "      <td>15.987049</td>\n",
              "      <td>115.681140</td>\n",
              "      <td>7.800741</td>\n",
              "      <td>0.322408</td>\n",
              "      <td>13.829831</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-22.000000</td>\n",
              "      <td>-5000.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>-150.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.375000</td>\n",
              "      <td>0.243250</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>32.300000</td>\n",
              "      <td>0.368000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.750000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>129.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.611500</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.329000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e645f850-9e2a-480f-96dd-8ba4bd86b8df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e645f850-9e2a-480f-96dd-8ba4bd86b8df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e645f850-9e2a-480f-96dd-8ba4bd86b8df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "db.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dbbcb5d-06c7-476d-b310-f260b4136010",
      "metadata": {
        "id": "3dbbcb5d-06c7-476d-b310-f260b4136010"
      },
      "source": [
        "We can see the count, mean, standard deviation, min, max and quartiles of numerical values.\n",
        "\n",
        "Now let's try info method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7f81ebff-4df0-45ae-89ee-d14664f9c9c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f81ebff-4df0-45ae-89ee-d14664f9c9c9",
        "outputId": "e815d05c-0323-4512-f59b-7f41636ef37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               635 non-null    float64\n",
            " 1   Glucose                   654 non-null    float64\n",
            " 2   BloodPressure             680 non-null    float64\n",
            " 3   SkinThickness             624 non-null    float64\n",
            " 4   Insulin                   680 non-null    float64\n",
            " 5   BMI                       684 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  590 non-null    float64\n",
            " 7   Age                       655 non-null    float64\n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(8), int64(1)\n",
            "memory usage: 54.1 KB\n"
          ]
        }
      ],
      "source": [
        "db.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3914e644-2a2d-408a-bb71-8709b066e546",
      "metadata": {
        "id": "3914e644-2a2d-408a-bb71-8709b066e546"
      },
      "source": [
        "As you can see, pregnancies, glucose and temp columns have some null values, which should be handled in preprocessing phase. Let's see the percentage of null values for each feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d5ea8bb9-7dde-4962-9a53-ec6dc850f45c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ea8bb9-7dde-4962-9a53-ec6dc850f45c",
        "outputId": "2f1f3528-9262-413d-988b-c0936775d0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of NaN values in dataset per feature:\n",
            "Pregnancies : 17.317708333333332 %\n",
            "Glucose : 14.84375 %\n",
            "BloodPressure : 11.458333333333334 %\n",
            "SkinThickness : 18.75 %\n",
            "Insulin : 11.458333333333334 %\n",
            "BMI : 10.9375 %\n",
            "DiabetesPedigreeFunction : 23.177083333333332 %\n",
            "Age : 14.713541666666666 %\n",
            "Outcome : 0.0 %\n"
          ]
        }
      ],
      "source": [
        "print('Percentage of NaN values in dataset per feature:')\n",
        "for feature in db:\n",
        "    print(feature, \":\", db[feature].isnull().sum() * 100 / len(db), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911ea6cc-b7e3-4a4f-9e78-1a67e7533101",
      "metadata": {
        "id": "911ea6cc-b7e3-4a4f-9e78-1a67e7533101"
      },
      "source": [
        "Finally, we'll plot histograms and KDE to see the distribution of each feature:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns \n",
        "sns.heatmap(db.corr())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "oyc6F0Xqzcux",
        "outputId": "5548d249-a610-4d7c-c874-53a02f6c8735"
      },
      "id": "oyc6F0Xqzcux",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb611fcda00>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(db.nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "iAzwDnyD0_TZ",
        "outputId": "50c28b3f-c0ad-4480-ca3d-d7e4204f5644"
      },
      "id": "iAzwDnyD0_TZ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb60f6b1d30>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(db, vars = db.columns[0:-1], hue='Outcome')"
      ],
      "metadata": {
        "id": "5a3FRo5S31PO"
      },
      "id": "5a3FRo5S31PO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hexbin(x, y, color, max_series=None, min_series=None, **kwargs):\n",
        "    cmap = sns.light_palette(color, as_cmap=True)\n",
        "    ax = plt.gca()\n",
        "    xmin, xmax = min_series[x.name], max_series[x.name]\n",
        "    ymin, ymax = min_series[y.name], max_series[y.name]\n",
        "    plt.hexbin(x, y, gridsize=15, cmap=cmap, extent=[xmin, xmax, ymin, ymax], **kwargs)\n",
        "\n",
        "g = sns.PairGrid(db, hue='Outcome')\n",
        "g.map_diag(plt.hist)\n",
        "g.map_lower(plt.scatter, alpha=0.5)\n",
        "g.map_upper(hexbin, min_series=db.min(), max_series=db.max(), alpha=0.5)\n"
      ],
      "metadata": {
        "id": "D8k3RT7E5AiI"
      },
      "id": "D8k3RT7E5AiI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8b0e1a-ba7e-4948-b200-9b0f87813bfa",
      "metadata": {
        "id": "4c8b0e1a-ba7e-4948-b200-9b0f87813bfa"
      },
      "outputs": [],
      "source": [
        "from pandas.api.types import is_numeric_dtype\n",
        "for feature in db:\n",
        "    if feature != 'pregnancies' and feature != 'age':\n",
        "        plt.figure()\n",
        "        plt.xticks(rotation = 45)\n",
        "        if is_numeric_dtype(db[feature]):\n",
        "            plt.title('KDE of {}'.format(feature))\n",
        "            db[feature].plot.kde()\n",
        "            plt.ylabel('Density')\n",
        "        else:\n",
        "            plt.title('Histogram of {}'.format(feature))\n",
        "            db[feature].hist()\n",
        "            plt.ylabel('Frequency')\n",
        "        plt.xlabel(feature)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b414c4-8244-47e0-b349-ece87048f345",
      "metadata": {
        "id": "26b414c4-8244-47e0-b349-ece87048f345"
      },
      "source": [
        "As you can see, the distribution of numerical values follows a Guassian distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2977300e-4e45-4ea8-b54c-bc69b1d01199",
      "metadata": {
        "id": "2977300e-4e45-4ea8-b54c-bc69b1d01199"
      },
      "source": [
        "Before doing anything new, let's seperate X and y dataframes from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e9a181-8018-4dce-b0e8-cfec87d03b12",
      "metadata": {
        "id": "b8e9a181-8018-4dce-b0e8-cfec87d03b12"
      },
      "outputs": [],
      "source": [
        "X, y = db.drop(['Outcome'], axis=1, inplace=False) , db['Outcome']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66ad507-a05e-46d6-866b-45e09b5d4c04",
      "metadata": {
        "id": "d66ad507-a05e-46d6-866b-45e09b5d4c04"
      },
      "source": [
        "## Phase 1: Preprocessing\n",
        "To remove NaN values from the dataframe, the most frequent value of each column is registered as the replaced value.\n",
        "The first method is imputation Using (Most Frequent) or (Zero/Constant) Values:\n",
        "This method is used for categorical features, but it doesnâ€™t factor the correlations between features and it can introduce bias in the data. Another possible method is imputation Using (Mean/Median) Values which is used for numerical values.\n",
        "Another approach is to drop the entire column or row, which leads to losing some data. Final approach is to predict the NaN values based on other features(like using Linear Regression or other machine learning methods), which can introduce bias in data. \n",
        "\n",
        "Let's fill NaN values based on first and second approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba194b1-2ae7-40e3-b176-3ce649683129",
      "metadata": {
        "id": "0ba194b1-2ae7-40e3-b176-3ce649683129"
      },
      "outputs": [],
      "source": [
        "for i in X.columns[X.isnull().any(axis=0)]:\n",
        "    if is_numeric_dtype(X[i]):\n",
        "        X[i].fillna(X[i].mean(), inplace=True)\n",
        "    elif i != 'y':\n",
        "        X[i].fillna(X[i].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05a057a-70dd-4c9c-97fc-893f0c50575e",
      "metadata": {
        "id": "a05a057a-70dd-4c9c-97fc-893f0c50575e"
      },
      "outputs": [],
      "source": [
        "X.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86279bb1-6364-4fcc-a29d-624c64a69ee1",
      "metadata": {
        "id": "86279bb1-6364-4fcc-a29d-624c64a69ee1"
      },
      "source": [
        "Now let's try normalization/standardization. Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling. Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
        "\n",
        "Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
        "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n",
        "\n",
        "Scaling avoids raw data and various problems of datasets by creating new values and maintaining general distribution as well as a ratio in data. Further, it also improves the performance and accuracy of machine learning models using various techniques and algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ded5e383-72ed-4ea6-bdaa-dbdce9dee21a",
      "metadata": {
        "id": "ded5e383-72ed-4ea6-bdaa-dbdce9dee21a"
      },
      "source": [
        "Since our numerical values follow a guassian distribution, we use standardization method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16e713c-0e94-4c9a-afba-7232ba720e66",
      "metadata": {
        "id": "d16e713c-0e94-4c9a-afba-7232ba720e66"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "\n",
        "for feature in X:\n",
        "    if is_numeric_dtype(X[feature]):\n",
        "        scaled = StandardScaler().fit(X[[feature]])\n",
        "        X[feature] = scaled.transform(X[[feature]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ff2fcf-2dca-46cf-9d32-20655d7fc8fc",
      "metadata": {
        "id": "11ff2fcf-2dca-46cf-9d32-20655d7fc8fc"
      },
      "source": [
        "Now let's work with categorical values. There are several methods to convert categorical data into numerical data:\n",
        "- Label Encoding: Assigining numbers to each categorical value.\n",
        "- One Hot Encoding: Adding new columns for each categorical value and assigining a binary vector to each row.\n",
        "\n",
        "Label Encoding isn't a good approach when we have no particular ordering in our categories. In such cases, we can use One Hot Encoding which takes a lot of memory since it is adding a new column for each new category. Since total number of possible values for 'key' and 'mode' isn't that high and there isn't an obvious order in the data, we'll use one-hot encoding for these two features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e53167c-070f-4335-918a-bb3f7cfe2368",
      "metadata": {
        "id": "2e53167c-070f-4335-918a-bb3f7cfe2368"
      },
      "source": [
        "Let's see the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f43c14-8952-4895-af83-6484da07da38",
      "metadata": {
        "id": "90f43c14-8952-4895-af83-6484da07da38"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caae51bc-7d09-4e94-99f2-06a32859daac",
      "metadata": {
        "id": "caae51bc-7d09-4e94-99f2-06a32859daac"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fa17f1e5-f03a-40a4-a64f-0090642e4c2c",
      "metadata": {
        "id": "fa17f1e5-f03a-40a4-a64f-0090642e4c2c"
      },
      "source": [
        "Let's see the gain for each feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2ecc08-b096-4989-ab5f-efa4d263ea9a",
      "metadata": {
        "id": "9e2ecc08-b096-4989-ab5f-efa4d263ea9a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "info = mutual_info_classif(X, y)\n",
        "ig = pd.Series(info)\n",
        "ig.index = X.columns\n",
        "ig.sort_values(ascending = False, inplace = True)\n",
        "\n",
        "plt.title('Mutual information with respect to features')\n",
        "ig.plot.bar(figsize=(25,20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "638c3678-e656-4fd5-bb32-116e187b8420",
      "metadata": {
        "id": "638c3678-e656-4fd5-bb32-116e187b8420"
      },
      "source": [
        "Feature selection is the process of reducing the number of input variables when developing a predictive model.\n",
        "\n",
        "It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
        "\n",
        "As you can see from the plot, some of the features doesn't give that much gain and we can get rid of them. We choose to keep the first 11 features in the plot to avoid overfitting and undefitting and get a good performance(we ignore minor since if a mode isn't major, it's minor and we don't want to save redundant data). Let's do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc7d100-e936-48e2-a973-e564ae4343f5",
      "metadata": {
        "id": "abc7d100-e936-48e2-a973-e564ae4343f5"
      },
      "outputs": [],
      "source": [
        "for column in X.columns:\n",
        "    if column not in ig[:11]:\n",
        "        X.drop(columns=[column], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7295bf9b-4318-4bf0-bece-04c020c334d8",
      "metadata": {
        "id": "7295bf9b-4318-4bf0-bece-04c020c334d8"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea9f537-2443-4e54-b771-13150f8f350b",
      "metadata": {
        "id": "3ea9f537-2443-4e54-b771-13150f8f350b"
      },
      "source": [
        "## Phase 2: Model Training, Evaluation and Hyper Parameter Tuning\n",
        "In the context of Machine Learning, the split of our modelling dataset into training and testing samples is probably one of the earliest pre-processing steps that we need to undertake. The creation of different samples for training and testing helps us evaluate model performance.\n",
        "ome common inferences that can be derived on dataset split include:\n",
        "\n",
        "If there are several hyperparameters to tune, the machine learning model requires a larger validation set to optimize the model performance. Similarly, if the model has fewer or no hyperparameters, it would be easy to validate the model using a small set of data.\n",
        "If a model use case is such that a false prediction can drastically hamper the model performanceâ€”like falsely predicting cancerâ€”itâ€™s better to validate the model after each epoch to make the model learn varied scenarios.\n",
        "With the increase in the dimension/features of the data, the hyperparameters of the neural network functions also increase making the model more complex. In these scenarios, a large split of data should be kept in training set with a validation set.\n",
        "\n",
        "The truth is-\n",
        "\n",
        "There is no optimal split percentage.\n",
        "\n",
        "One has to come to a split percentage that suits the requirements and meets the modelâ€™s needs. \n",
        "\n",
        "However, there are two major concerns while deciding on the optimum split:\n",
        "\n",
        "If there is less training data, the machine learning model will show high variance in training.\n",
        "With less testing data/validation data, your model evaluation/model performance statistic will have greater variance.\n",
        "\n",
        "Random sampling is the oldest and most popular method for dividing a dataset. As the name suggests, the dataset is shuffled, and samples are picked randomly and put in the train, validation, or the test set based on what percentage ratio is given by the user.\n",
        "\n",
        "However, this method has a significant drawback. Random sampling works optimally on class-balanced datasets, i.e., datasets with the more or less the same number of samples in every dataset category. In the case of class-imbalanced datasets, such a data splitting method may create a bias.\n",
        "\n",
        "Stratified sampling for splitting a dataset alleviates the problem of Random Sampling in datasets with an imbalanced-class distribution. Here, the distribution of classes in each of the train, validation, and test sets is preserved.\n",
        "\n",
        "Stratified sampling is, thus, a more fair way of data splitting, such that the machine learning model will be trained and validated on the same data distribution.\n",
        "\n",
        "Let's apply these knowledge using 75/25 split percentage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a454cd8-7f3f-4c90-8884-525b6f696203",
      "metadata": {
        "id": "5a454cd8-7f3f-4c90-8884-525b6f696203"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974f197c-fafe-4ed0-b9a1-9d007c7b3ecb",
      "metadata": {
        "id": "974f197c-fafe-4ed0-b9a1-9d007c7b3ecb"
      },
      "source": [
        "## KNN\n",
        "Let's fit KNN model with different parameters and see the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25289a93-9275-4af6-b14d-0dec9d2e1d4c",
      "metadata": {
        "id": "25289a93-9275-4af6-b14d-0dec9d2e1d4c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neighbours = range(1, 20)\n",
        "\n",
        "test = []\n",
        "train = []\n",
        "\n",
        "for i in neighbours:\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train,y_train)\n",
        "\n",
        "    train_pred= knn.predict(X_train)\n",
        "    train.append(accuracy_score(y_train, train_pred))\n",
        "\n",
        "    test_pred = knn.predict(X_test)\n",
        "    test.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(neighbours, train, label='Train')\n",
        "plt.plot(neighbours, test, label='Test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy according to n_neighbors')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b96d98b-1748-4250-83f1-e42c68c875ae",
      "metadata": {
        "id": "8b96d98b-1748-4250-83f1-e42c68c875ae"
      },
      "source": [
        "As you can see, for low neighbours we have high accuracy in train and low accuracy in test, which is a sign of overfitting. Best k, according to plot, is around 15-20. We choose k = 15 for the final model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a9173d-d77a-476c-89e8-6c2a8c6edd73",
      "metadata": {
        "id": "69a9173d-d77a-476c-89e8-6c2a8c6edd73"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "knn.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e74d01-a1a5-48af-89d1-0ed5038d5e82",
      "metadata": {
        "id": "32e74d01-a1a5-48af-89d1-0ed5038d5e82"
      },
      "source": [
        "## Decision Tree\n",
        "Let's fit Decision Tree model with different parameters and see the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1af203b-e238-4873-bf77-6b52f46dd4f1",
      "metadata": {
        "id": "b1af203b-e238-4873-bf77-6b52f46dd4f1"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "depths = range(1, 30)\n",
        "\n",
        "test = []\n",
        "train = []\n",
        "\n",
        "for i in depths:\n",
        "    tree = DecisionTreeClassifier(max_depth=i)\n",
        "    tree.fit(X_train,y_train)\n",
        "\n",
        "    train_pred= tree.predict(X_train)\n",
        "    train.append(accuracy_score(y_train, train_pred))\n",
        "\n",
        "    test_pred = tree.predict(X_test)\n",
        "    test.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(depths, train, label='Train')\n",
        "plt.plot(depths, test, label='Test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy according to depths')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3629324-c444-4d79-b641-52acea345752",
      "metadata": {
        "id": "e3629324-c444-4d79-b641-52acea345752"
      },
      "source": [
        "We choose depth = 5 to avoid underfitting and overfitting. Let's try other factor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb7c07f-1e5e-4ea6-967e-9e5163ad188d",
      "metadata": {
        "id": "9cb7c07f-1e5e-4ea6-967e-9e5163ad188d"
      },
      "outputs": [],
      "source": [
        "leaves = range(1, 30)\n",
        "\n",
        "test = []\n",
        "train = []\n",
        "\n",
        "for i in leaves:\n",
        "    tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=i)\n",
        "    tree.fit(X_train,y_train)\n",
        "\n",
        "    train_pred= tree.predict(X_train)\n",
        "    train.append(accuracy_score(y_train, train_pred))\n",
        "\n",
        "    test_pred = tree.predict(X_test)\n",
        "    test.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(depths, train, label='Train')\n",
        "plt.plot(depths, test, label='Test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy according to min_samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logistic regression\n"
      ],
      "metadata": {
        "id": "XB1a259IbCN6"
      },
      "id": "XB1a259IbCN6"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(ds_train, out_train)\n",
        "predictions = logisticRegr.predict(ds_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def check(x, y, model, name):\n",
        "    y_pred = model.predict(x)\n",
        "    print(classification_report(y, y_pred))\n",
        "    print('Accuracy for {} is {}'.format(name, accuracy_score(y, y_pred)))\n",
        "    print('--------')\n",
        "\n",
        "xs = [X_train, X_test]\n",
        "ys = [y_train, y_test]\n",
        "\n",
        "models = {\n",
        "    'tree': tree,\n",
        "    'knn': knn,\n",
        "'logistic regression': logisticRegr\n",
        "}\n",
        "\n",
        "for model in models.keys():\n",
        "    for _x, _y in zip(xs, ys):\n",
        "        check(_x, _y, models[model], model)"
      ],
      "metadata": {
        "id": "whF-TqpNazKm"
      },
      "id": "whF-TqpNazKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "80bdf0ba-a4fa-448a-9b73-12eb74b55b3a",
      "metadata": {
        "id": "80bdf0ba-a4fa-448a-9b73-12eb74b55b3a"
      },
      "source": [
        "We choose min_leaf = 15 to avoid underfitting and overfitting. Let's create the final model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcaaa74-8f7c-4490-a50a-c3e68854c241",
      "metadata": {
        "id": "2dcaaa74-8f7c-4490-a50a-c3e68854c241"
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=15)\n",
        "tree.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053e34ab-1db7-4104-b6d7-cfd3cc4c3452",
      "metadata": {
        "id": "053e34ab-1db7-4104-b6d7-cfd3cc4c3452"
      },
      "source": [
        "Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data. (Itâ€™s just like trying to fit undersized pants!) Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. In such cases, the rules of the machine learning model are too easy and flexible to be applied on such minimal data and therefore the model will probably make a lot of wrong predictions. Underfitting can be avoided by using more data and also reducing the features by feature selection. \n",
        "\n",
        "In a nutshell, Underfitting â€“ High bias and low variance \n",
        "\n",
        "Techniques to reduce underfitting: \n",
        "\n",
        "- Increase model complexity\n",
        "- Increase the number of features, performing feature engineering\n",
        "- Remove noise from the data.\n",
        "- Increase the number of epochs or increase the duration of training to get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58925012-d8d4-4100-a074-4f7a519519ab",
      "metadata": {
        "id": "58925012-d8d4-4100-a074-4f7a519519ab"
      },
      "source": [
        "Overfitting: \n",
        "A statistical model is said to be overfitted when we train it with a lot of data (just like fitting ourselves in oversized pants!). When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
        "\n",
        "In a nutshell, Overfitting â€“ High variance and low bias \n",
        "Techniques to reduce overfitting:\n",
        "\n",
        "- Increase training data.\n",
        "- Reduce model complexity.\n",
        "- Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
        "- Ridge Regularization and Lasso Regularization\n",
        "- Use dropout for neural networks to tackle overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df395658-3026-44db-ae26-1390ef097992",
      "metadata": {
        "id": "df395658-3026-44db-ae26-1390ef097992"
      },
      "source": [
        "In our models, we try to tune hyperparameters to avoid overfitting and underfitting. As you can see, the accuracy in both models are nearly the same for test and train."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6779f85-6a59-4d58-ae4a-6372df85a0dd",
      "metadata": {
        "id": "f6779f85-6a59-4d58-ae4a-6372df85a0dd"
      },
      "source": [
        "## Accuracy\n",
        "Itâ€™s the ratio of the correctly labeled subjects to the whole pool of subjects.\n",
        "Accuracy is the most intuitive one.\n",
        "Accuracy answers the following question: How many students did we correctly label out of all the students?\n",
        "Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
        "numerator: all correctly labeled subject (All trues)\n",
        "denominator: all subjects\n",
        "\n",
        "## Precision\n",
        "Precision is the ratio of the correctly +ve labeled by our program to all +ve labeled.\n",
        "Precision answers the following: How many of those who we labeled as diabetic are actually diabetic?\n",
        "Precision = TP/(TP+FP)\n",
        "numerator: +ve labeled diabetic people.\n",
        "denominator: all +ve labeled by our program (whether theyâ€™re diabetic or not in reality).\n",
        "\n",
        "## Recall (aka Sensitivity)\n",
        "Recall is the ratio of the correctly +ve labeled by our program to all who are diabetic in reality.\n",
        "Recall answers the following question: Of all the people who are diabetic, how many of those we correctly predict?\n",
        "Recall = TP/(TP+FN)\n",
        "numerator: +ve labeled diabetic people.\n",
        "denominator: all people who are diabetic (whether detected by our program or not)\n",
        "\n",
        "## F1-score (aka F-Score / F-Measure)\n",
        "F1 Score considers both precision and recall.\n",
        "It is the harmonic mean(average) of the precision and recall.\n",
        "F1 Score is best if there is some sort of balance between precision (p) & recall (r) in the system. Oppositely F1 Score isnâ€™t so high if one measure is improved at the expense of the other.\n",
        "For example, if P is 1 & R is 0, F1 score is 0.\n",
        "F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af39acfa-6a4d-48a3-9652-1a142910276c",
      "metadata": {
        "id": "af39acfa-6a4d-48a3-9652-1a142910276c"
      },
      "source": [
        "Let's see these metrics for our models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2354fae0-35fe-4dcb-904f-5bab7363c409",
      "metadata": {
        "id": "2354fae0-35fe-4dcb-904f-5bab7363c409"
      },
      "outputs": [],
      "source": [
        "def check(x, y, model, name):\n",
        "    y_pred = model.predict(x)\n",
        "    print(classification_report(y, y_pred))\n",
        "    print('Accuracy for {} is {}'.format(name, accuracy_score(y, y_pred)))\n",
        "    print('--------')\n",
        "\n",
        "xs = [X_train, X_test]\n",
        "ys = [y_train, y_test]\n",
        "\n",
        "models = {\n",
        "    'tree': tree,\n",
        "    'knn': knn\n",
        "}\n",
        "\n",
        "for model in models.keys():\n",
        "    for _x, _y in zip(xs, ys):\n",
        "        check(_x, _y, models[model], model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede8e979-bb3b-40dd-90f7-fdbc7a492dcf",
      "metadata": {
        "id": "ede8e979-bb3b-40dd-90f7-fdbc7a492dcf"
      },
      "source": [
        "Preprocessing is a big step in machine learning. If we dropped tuples with NaN values, we could probably lead to lower accuracy(since we would have less data)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f2cd2f-84de-4d2b-a93a-49c5cf83c6a4",
      "metadata": {
        "id": "55f2cd2f-84de-4d2b-a93a-49c5cf83c6a4"
      },
      "source": [
        "## Phase 3: Ensemble Methods\n",
        "\n",
        "We use random forest in this phase. Let's see the impact of different hyperparams in the result. Let's go!\n",
        "\n",
        "The hyper parameters of Random Forest:\n",
        "\n",
        "\n",
        "- n_estimators: The number of trees in the forest.\n",
        "- criterion: The function to measure the quality of a split. \n",
        "- max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "- min_samples_split: The minimum number of samples required to split an internal node:\n",
        "- bootstrap: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f380d54-8e44-4131-a548-d1014ef94bff",
      "metadata": {
        "id": "5f380d54-8e44-4131-a548-d1014ef94bff"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "depths = range(1, 30)\n",
        "\n",
        "test = []\n",
        "train = []\n",
        "\n",
        "for i in depths:\n",
        "    forest = RandomForestClassifier(max_depth=i)\n",
        "    forest.fit(X_train,y_train)\n",
        "\n",
        "    train_pred= forest.predict(X_train)\n",
        "    train.append(accuracy_score(y_train, train_pred))\n",
        "\n",
        "    test_pred = forest.predict(X_test)\n",
        "    test.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(depths, train, label='Train')\n",
        "plt.plot(depths, test, label='Test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy according to depths')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4b7460-b0e0-45bf-94ae-01791402a77d",
      "metadata": {
        "id": "ec4b7460-b0e0-45bf-94ae-01791402a77d"
      },
      "source": [
        "We choose max_depth = 12 to avoid underfitting and overfitting. Let's continue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751d610d-1ea6-4c78-9e9e-0f99dd7e8c25",
      "metadata": {
        "id": "751d610d-1ea6-4c78-9e9e-0f99dd7e8c25"
      },
      "outputs": [],
      "source": [
        "estimators = range(1, 30)\n",
        "\n",
        "test = []\n",
        "train = []\n",
        "\n",
        "for i in estimators:\n",
        "    forest = RandomForestClassifier(max_depth=12, n_estimators=i)\n",
        "    forest.fit(X_train,y_train)\n",
        "\n",
        "    train_pred= forest.predict(X_train)\n",
        "    train.append(accuracy_score(y_train, train_pred))\n",
        "\n",
        "    test_pred = forest.predict(X_test)\n",
        "    test.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(depths, train, label='Train')\n",
        "plt.plot(depths, test, label='Test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy according to estimators')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d4f61a6-6093-4ca2-ac9f-b34d8c2316fd",
      "metadata": {
        "id": "0d4f61a6-6093-4ca2-ac9f-b34d8c2316fd"
      },
      "source": [
        "We choose n_estimators = 23 to avoid underfitting and overfitting. Let's continue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc290f2-a082-448f-a639-f9e273083695",
      "metadata": {
        "id": "1bc290f2-a082-448f-a639-f9e273083695"
      },
      "outputs": [],
      "source": [
        "leaves = range(1, 30)\n",
        "\n",
        "test = []\n",
        "train = []\n",
        "\n",
        "for i in leaves:\n",
        "    forest = RandomForestClassifier(max_depth=12, n_estimators=23, min_samples_leaf=i)\n",
        "    forest.fit(X_train,y_train)\n",
        "\n",
        "    train_pred= forest.predict(X_train)\n",
        "    train.append(accuracy_score(y_train, train_pred))\n",
        "\n",
        "    test_pred = forest.predict(X_test)\n",
        "    test.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(depths, train, label='Train')\n",
        "plt.plot(depths, test, label='Test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy according to min_samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cebfd70-8142-480c-af00-01bcb60443b0",
      "metadata": {
        "id": "3cebfd70-8142-480c-af00-01bcb60443b0"
      },
      "source": [
        "We choose min_samples = 10 to avoid underfitting and overfitting. Let's finalize the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1723f797-da41-4e8c-8cc3-b666ae95f199",
      "metadata": {
        "id": "1723f797-da41-4e8c-8cc3-b666ae95f199"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(max_depth=12, n_estimators=23, min_samples_leaf=10)\n",
        "forest.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc9a7835-5190-48b8-8840-21066317219a",
      "metadata": {
        "id": "dc9a7835-5190-48b8-8840-21066317219a"
      },
      "source": [
        "Let's check the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeddfccd-e352-4742-b031-8712f3896e64",
      "metadata": {
        "id": "eeddfccd-e352-4742-b031-8712f3896e64"
      },
      "outputs": [],
      "source": [
        "for _x, _y in zip(xs, ys):\n",
        "    check(_x, _y, forest, 'forest')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b8bc6f-c8fb-40a9-8a82-8dcfa0d74302",
      "metadata": {
        "id": "a0b8bc6f-c8fb-40a9-8a82-8dcfa0d74302"
      },
      "source": [
        "Let's see the confusion matrix for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4390cb93-7683-4e0a-adb0-e722d60eedb3",
      "metadata": {
        "id": "4390cb93-7683-4e0a-adb0-e722d60eedb3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "matrix = ConfusionMatrixDisplay.from_estimator(forest, X_test, y_test, cmap=plt.cm.Reds, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2658d346-188d-4cae-a8d8-5b36e52d64c3",
      "metadata": {
        "id": "2658d346-188d-4cae-a8d8-5b36e52d64c3"
      },
      "source": [
        "# Conclusion\n",
        "The process of choosing the right machine learning model to solve a problem can be time consuming if not approached strategically.\n",
        "\n",
        "Step 1: Align the problem with potential data inputs that should be considered for the solution. This step requires help from data scientists and experts who have a deep understanding of the problem.\n",
        "\n",
        "Step 2: Collect data, format it and label the data if necessary. This step is typically led by data scientists, with help from data wranglers.\n",
        "\n",
        "Step 3: Chose which algorithm(s) to use and test to see how well they perform. This step is usually carried out by data scientists.\n",
        "\n",
        "Step 4: Continue to fine tune outputs until they reach an acceptable level of accuracy. This step is usually carried out by data scientists with feedback from experts who have a deep understanding of the problem."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5Kp20bLQPG2"
      },
      "id": "o5Kp20bLQPG2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}